{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nfrom wordcloud import WordCloud, STOPWORDS\nfrom string import punctuation\nfrom collections import Counter\nfrom nltk.stem.porter import *\nimport itertools\nimport re\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\nfrom torch import optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available, CPU is being used instead\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\nprint(\"The csv shape: \", str(train_df.shape))\ntrain_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Example text: \", str(train_df[\"text\"][1]), \"\\n\", \"Target: \", str(train_df[\"target\"][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train_df.isnull().sum().sort_values(ascending = False)\npercent = (train_df.isnull().sum())/(train_df.isnull().count()).sort_values(ascending = False)\n\nmissing_data = pd.concat([total, percent], axis = 1, keys = [\"total\", \"percent\"])\nmissing_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop([\"keyword\", \"location\", \"id\"], axis = 1)\ntest_df = test_df.drop([\"keyword\", \"location\", \"id\"], axis = 1)\nprint(\"Keyword, Location, and id are all dropped successfully\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase = \"Hello,,?? This is my Tweet number 17!!!. I also wanted or been wanting you to try reading this tweet https://www.youtube.com and \\\nalso be writing or seeing https://relentless.com. This is my 28th and and maybe 32nd time seeing this. #ThisisFun\"\npunct_list = set(punctuation)\n\n\ndef remove_punct(text):\n    \n    new_text = \"\".join(ch for ch in text if ch not in punct_list)\n    return new_text\n\ndef remove_stopwords(text):\n    \n    text_split = text.split(\" \")\n    text = [word for word in text_split if word not in STOPWORDS]\n    return text\n\ndef remove_http(text_list):\n    \n    new_text = [word for word in text_list if word.find(\"http\") == -1]\n    return new_text\n\ndef stem_porter(text_list):\n    \n    stemmer = PorterStemmer()\n    \n    new_text = [stemmer.stem(word) for word in text_list]\n    return new_text\n\ndef change_number(text_list):\n    \n    new_text = []\n    for word in text_list:\n        if (bool(re.search(r'\\d', word)) == False):\n            new_text.append(word)\n        else:\n            new_text.append(\"||Numeric||\")\n    \n    return new_text\n\nchange_number(stem_porter(remove_http(remove_stopwords(remove_punct(phrase.lower())))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explains what stem does\nprint(\"Before stem: \")\nprint(remove_http(remove_stopwords(remove_punct(phrase.lower()))))\n\nprint(\"After stem: \")\nprint(stem_porter(remove_http(remove_stopwords(remove_punct(phrase.lower())))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text):\n    \n    text = change_number(stem_porter(remove_http(remove_stopwords(remove_punct(text.lower())))))\n    return text\n\npreprocess_text(phrase)\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: preprocess_text(x))\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: preprocess_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The new preprocessed data\nprint(train_df[\"text\"])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Turn the text into numbers\n\nfreq = {}\nfor row in train_df[\"text\"]:\n    \n    for word in row:\n        if word in freq:\n            freq[word] += 1\n        else:\n            freq[word] = 1\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_freq = sorted(freq, key = freq.get, reverse = True)\nsorted_freq.remove('')\n\n#Now we have to tokenize\nvocab2int = {word: ii + 1 for ii, word in enumerate(sorted_freq)}\nint2vocab = {ii + 1: word for ii, word in enumerate(sorted_freq)}\n\nprint(dict(itertools.islice(vocab2int.items(), 100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_list = change_number(stem_porter(remove_http(remove_stopwords(remove_punct(phrase.lower())))))\nprint(\"Before: \", str(phrase_list))\ndef tokenize_text(text_list):\n    \n    int_text = []\n    for word in text_list:\n        try:\n            int_text.append(vocab2int[word])\n        except:\n            int_text.append(0)\n            \n    return int_text\n\nprint(\"After: \", str(tokenize_text(phrase_list)))\n\nprint(\"Second test: \")\ncommon_list = [\"fire\", \"bomb\", \"somemorerandom,,!!\", \"im\", \"first\", \"jibberish\", \"the\", \"||Numeric||\"]\nprint(tokenize_text(common_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(lambda x: tokenize_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 0\nfor tweet in train_df[\"text\"]:\n    if len(tweet) >= max_length:\n        max_length = len(tweet)\n\n        \nprint(\"Max Length: \", str(max_length))    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_text(int_list, sequence_length = 52):\n    \n    padded_list = np.zeros((sequence_length), dtype = int)\n    padded_list[-len(int_list):] = np.array(int_list)[:sequence_length]\n    \n    return padded_list\n\n\nint_list = tokenize_text(phrase_list)\nprint(pad_text(int_list))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have some rows we need to drop or else the pad won't work\nlist_of_empty_rows = []\nfor ii, tweet in enumerate(train_df[\"text\"]):\n    if len(tweet) == 0:\n        list_of_empty_rows.append(ii)\n\nprint(list_of_empty_rows)     \ntrain_df = train_df.drop(list_of_empty_rows)\nprint(\"Rows have been dropped\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(lambda x: pad_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"First Review\")\nprint(train_df[\"text\"][0][:])\n\nprint(\"Next Review\")\nprint(train_df[\"text\"][1028][:])\n\nprint(\"Next Review\")\nprint(train_df[\"text\"][456][:])\n\nprint(\"Length of review: \", str(len(train_df[\"text\"][906])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now I have to create a generator and also split into Valid and Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text(csv_column):\n    listed_data = []\n    for row in csv_column:\n        listed_data.append(row)\n        \n    return listed_data\n\ndef get_target(csv_column):\n    listed_data = []\n    for row in csv_column:\n        listed_row = [row]\n        listed_data.append(listed_row)\n    return listed_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = get_text(train_df[\"text\"])\ntrain_y = get_target(train_df[\"target\"])\n\nassert len(train_y) == len(train_x)\nprint(len(train_x))\nprint(len(train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Can use this method to split data if needed\n\n    def split_data(train_x, train_y):\n    \n    split_index = int(len(train_x) * 0.995)\n    train_x, valid_x = train_x[:split_index], train_x[split_index:]\n    train_y, valid_y = train_y[:split_index], train_y[split_index:]\n    \n    return train_x, train_y, valid_x, valid_y \n\n```  train_x, train_y, valid_x, valid_y = split_data(train_x, train_y) ``` "},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(train_x) == len(train_y)\nprint(len(train_x))\n#Uncomment these lines to create datalaoder for validation\n# assert len(valid_x) == len(valid_y)\n# print(len(valid_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataloader(train_x, train_y, batch_size = 30):\n    \n    #Make sure to convert from Numpy to Torch Tensor\n    train_dataset = TensorDataset(torch.LongTensor(train_x), torch.FloatTensor(train_y))\n    \n    train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size, drop_last = True)\n    \n    return train_loader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets create a dataloader\ntrain_loader = create_dataloader(train_x[:7595], train_y[:7595])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at the batches of data\ntrain_loader_iter = iter(train_loader)\nnext(train_loader_iter)[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we are going to define the model class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n    \n    def __init__(self,vocab_size, embedding_dim, hidden_size, n_layers):\n        super(LSTM, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm1 = nn.LSTM(embedding_dim, self.hidden_size, num_layers = self.n_layers, dropout = 0.2, batch_first = True)\n        self.fc1 = nn.Linear(self.hidden_size, 1)\n        \n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        \n        batch_size = x.size(0)\n        sequence_len = x.size(1)\n        \n        embeddings = self.embedding(x)\n        lstm_out, hidden = self.lstm1(embeddings, hidden)\n        output = lstm_out.contiguous().view(-1, self.hidden_size)\n        \n        output = self.sigmoid(self.fc1(output))\n        output = output.reshape(batch_size, sequence_len, -1)\n        output = output[:, -1]\n        \n        \n        return hidden, output\n    \n    def init_hidden(self, batch_size):\n        \n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda())\n            \n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_size).zero_())\n            \n        return hidden\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we have to define the train function\n* forward and backprop function\n* The final train function "},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_and_backprop(rnn, optimizer, tweet, target, criterion, hidden):\n    \n    if train_on_gpu:\n        rnn.cuda()\n    \n    hidden = ([each.data for each in hidden])\n    optimizer.zero_grad()\n    \n    hidden, output = rnn(tweet, hidden)\n    loss = criterion(output, target)\n    \n    loss.backward()\n    \n    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n    optimizer.step()\n    \n    loss_for_batch = loss.item()\n    \n    return loss_for_batch, hidden\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(rnn, epochs, optimizer, criterion, batch_size, train_loader):\n    \n    rnn.train()\n    if train_on_gpu:\n        rnn = rnn.cuda()\n        \n    for epoch in range(1, epochs + 1):\n        \n        hidden = rnn.init_hidden(batch_size)\n        \n        train_loss = 0\n        \n        for batch_i, (tweet, target) in enumerate(train_loader):\n            \n            if train_on_gpu:\n                tweet = tweet.cuda()\n                target = target.cuda() \n            \n            batch_loss, hidden = forward_and_backprop(rnn, optimizer, tweet, target, criterion, hidden)\n            train_loss += batch_loss\n        \n        \n        print(\"Epoch Number: \", str(epoch)) \n        print(\"Train Loss: \", str(train_loss))\n        \n    \n    \n        \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now Let us define the hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nbatch_size = 30\nlr = 0.001\n\nvocab_size = len(vocab2int)\nembedding_dim = 230\nhidden_size = 250\nnum_layers = 2\n\nrnn = LSTM(vocab_size, embedding_dim, hidden_size, num_layers) \n\noptimizer = optim.Adam(rnn.parameters(), lr = lr)\ncriterion = nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(rnn, epochs, optimizer, criterion, batch_size, train_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we are going to decide the predict model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(net, test_review):\n    \n    assert len(test_review) > 0\n    \n    net.eval()\n    preprocessed_data = tokenize_text(preprocess_text(test_review))\n    \n    assert len(test_review) > 0\n    \n    padded_data = pad_text(preprocessed_data)\n    padded_data = padded_data.reshape(1, -1)\n    padded_data = torch.from_numpy(padded_data)\n   \n    \n    batch_size = padded_data.size(0)\n    hidden = net.init_hidden(batch_size)\n    \n    if train_on_gpu:\n        padded_data = padded_data.cuda()\n    \n    hidden, output = net(padded_data, hidden)\n    \n    print(test_review)\n    print(\"Unrounded Answer: \", output)\n    \n    answer = np.round(output.cpu().detach().numpy())\n    if answer == 1:\n        print(\"Call in immediate emergency at location\")\n    else:\n        print(\"General Commentary\")\n    \n    \n\npredict(rnn, \"Breaking News: Flooding on streets\")\npredict(rnn, \"Fire ravaged houses next to me and are approaching me\")\npredict(rnn, \"We the best music. We just chillin #Ballin\")\npredict(rnn, \"Smoke in the air. It smells like smoke #Fire\")\npredict(rnn, \"Smoke in the air. It smells like smoke #SaySikeRightNow\")\npredict(rnn, \"I hear strange noises. The wall is shaking\")\npredict(rnn, \"I am on fire with playing this game #TheGOAT\")\npredict(rnn, \"High winds very high winds the ground is shaking\")\npredict(rnn, \"I dont do domestic violence\")\npredict(rnn, \"The houses next to us have burned to pure ash\")\npredict(rnn, \"Breaking News: High Water levels threatening Silicon Valley\")\npredict(rnn, \"Oh no what is happening. Flooding is affected my House\")\npredict(rnn, \"Oh no what is happening. Flooding is affected my House\")\npredict(rnn, \"there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\")\npredict(rnn, \"This fire is huge. How I am I supposed to put this out myself bruh\")\npredict(rnn, \"I can a broken car set ablaze on the side of street calling 911\")\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets test on some test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(rnn, \"ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF SE VISTA TER GRESHAM OR [Gresham Police #PG15000044357...\")\npredict(rnn, \"@DaveOshry @Soembie So if I say that I met her by accident this week- would you be super jelly Dave?...\")\npredict(rnn, \"We're shaking...It's an earthquake\")\npredict(rnn, \"We are still living in the aftershock of Hiroshima people are still the scars of history.' - Edward...\")\npredict(rnn, \"320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/THyzOMVWU0 | @djicemoon | #Dubstep #TrapMusic #DnB #EDM ...\")\npredict(rnn, \"#UPDATE: Picture from the Penn Twp. airplane accident. http://t.co/6JfgDnZRlC\")\npredict(rnn, \"@thugIauren I had myself on airplane mode by accident ??\")\npredict(rnn, \"Typhoon Soudelor kills 28 in China and Taiwan\")\npredict(rnn, \"No I don't like cold!\")\npredict(rnn, \"Not a diss song. People will take 1 thing and run with it. Smh it's an eye opener though. He is abou...\")\npredict(rnn, \"Just got to love burning your self on a damn curling wand... I swear someone needs to take it away f...\")\npredict(rnn, \"I hate badging shit in accident\")\npredict(rnn, \"Horrible Accident Man Died In Wings of AirplaneåÊ(29-07-2015) http://t.co/5ZRKZdhODe\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(rnn, \"I am just chillin \")\npredict(rnn, \"Ballin usual like the man\")\npredict(rnn, \"Her house paint looks fire\")\npredict(rnn, \"Whoopsy daisy, accident\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}