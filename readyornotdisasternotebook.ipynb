{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nfrom wordcloud import WordCloud, STOPWORDS\nfrom string import punctuation\nfrom collections import Counter\nfrom nltk.stem.porter import *\nimport itertools\nimport re\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\nfrom torch import optim","execution_count":1,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available, CPU is being used instead\")","execution_count":2,"outputs":[{"output_type":"stream","text":"GPU is available\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\nprint(\"The csv shape: \", str(train_df.shape))\ntrain_df.head()\n","execution_count":3,"outputs":[{"output_type":"stream","text":"The csv shape:  (7613, 5)\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Example text: \", str(train_df[\"text\"][1]), \"\\n\", \"Target: \", str(train_df[\"target\"][1]))","execution_count":5,"outputs":[{"output_type":"stream","text":" Example text:  Forest fire near La Ronge Sask. Canada \n Target:  1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train_df.isnull().sum().sort_values(ascending = False)\npercent = (train_df.isnull().sum())/(train_df.isnull().count()).sort_values(ascending = False)\n\nmissing_data = pd.concat([total, percent], axis = 1, keys = [\"total\", \"percent\"])\nmissing_data\n","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"          total   percent\nlocation   2533  0.332720\nkeyword      61  0.008013\ntarget        0  0.000000\ntext          0  0.000000\nid            0  0.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>location</th>\n      <td>2533</td>\n      <td>0.332720</td>\n    </tr>\n    <tr>\n      <th>keyword</th>\n      <td>61</td>\n      <td>0.008013</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>text</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>id</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop([\"keyword\", \"location\", \"id\"], axis = 1)\ntest_df = test_df.drop([\"keyword\", \"location\", \"id\"], axis = 1)\nprint(\"Keyword, Location, and id are all dropped successfully\")\n\n","execution_count":7,"outputs":[{"output_type":"stream","text":"Keyword, Location, and id are all dropped successfully\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase = \"Hello,,?? This is my Tweet number 17!!!. I also wanted or been wanting you to try reading this tweet https://www.youtube.com and \\\nalso be writing or seeing https://relentless.com. This is my 28th and and maybe 32nd time seeing this. #ThisisFun\"\npunct_list = set(punctuation)\n\n\ndef remove_punct(text):\n    \n    new_text = \"\".join(ch for ch in text if ch not in punct_list)\n    return new_text\n\ndef remove_stopwords(text):\n    \n    text_split = text.split(\" \")\n    text = [word for word in text_split if word not in STOPWORDS]\n    return text\n\ndef remove_http(text_list):\n    \n    new_text = [word for word in text_list if word.find(\"http\") == -1]\n    return new_text\n\ndef stem_porter(text_list):\n    \n    stemmer = PorterStemmer()\n    \n    new_text = [stemmer.stem(word) for word in text_list]\n    return new_text\n\ndef change_number(text_list):\n    \n    new_text = []\n    for word in text_list:\n        if (bool(re.search(r'\\d', word)) == False):\n            new_text.append(word)\n        else:\n            new_text.append(\"||Numeric||\")\n    \n    return new_text\n\nchange_number(stem_porter(remove_http(remove_stopwords(remove_punct(phrase.lower())))))","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"['hello',\n 'tweet',\n 'number',\n '||Numeric||',\n 'want',\n 'want',\n 'tri',\n 'read',\n 'tweet',\n 'write',\n 'see',\n '||Numeric||',\n 'mayb',\n '||Numeric||',\n 'time',\n 'see',\n 'thisisfun']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explains what stem does\nprint(\"Before stem: \")\nprint(remove_http(remove_stopwords(remove_punct(phrase.lower()))))\n\nprint(\"After stem: \")\nprint(stem_porter(remove_http(remove_stopwords(remove_punct(phrase.lower())))))","execution_count":9,"outputs":[{"output_type":"stream","text":"Before stem: \n['hello', 'tweet', 'number', '17', 'wanted', 'wanting', 'try', 'reading', 'tweet', 'writing', 'seeing', '28th', 'maybe', '32nd', 'time', 'seeing', 'thisisfun']\nAfter stem: \n['hello', 'tweet', 'number', '17', 'want', 'want', 'tri', 'read', 'tweet', 'write', 'see', '28th', 'mayb', '32nd', 'time', 'see', 'thisisfun']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text):\n    \n    text = change_number(stem_porter(remove_http(remove_stopwords(remove_punct(text.lower())))))\n    return text\n\npreprocess_text(phrase)\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: preprocess_text(x))\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: preprocess_text(x))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The new preprocessed data\nprint(train_df[\"text\"])\ntrain_df.head()","execution_count":11,"outputs":[{"output_type":"stream","text":"0       [deed, reason, earthquak, may, allah, forgiv, us]\n1            [forest, fire, near, la, rong, sask, canada]\n2       [resid, ask, shelter, place, notifi, offic, ev...\n3       [||Numeric||, peopl, receiv, wildfir, evacu, o...\n4       [got, sent, photo, rubi, alaska, smoke, wildfi...\n                              ...                        \n7608    [two, giant, crane, hold, bridg, collaps, near...\n7609    [ariaahrari, thetawniest, control, wild, fire,...\n7610    [||Numeric||, ||Numeric||, ||Numeric||, s, vol...\n7611    [polic, investig, ebik, collid, car, littl, po...\n7612    [latest, home, raze, northern, california, wil...\nName: text, Length: 7613, dtype: object\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"                                                text  target\n0  [deed, reason, earthquak, may, allah, forgiv, us]       1\n1       [forest, fire, near, la, rong, sask, canada]       1\n2  [resid, ask, shelter, place, notifi, offic, ev...       1\n3  [||Numeric||, peopl, receiv, wildfir, evacu, o...       1\n4  [got, sent, photo, rubi, alaska, smoke, wildfi...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[forest, fire, near, la, rong, sask, canada]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[||Numeric||, peopl, receiv, wildfir, evacu, o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Turn the text into numbers\n\nfreq = {}\nfor row in train_df[\"text\"]:\n    \n    for word in row:\n        if word in freq:\n            freq[word] += 1\n        else:\n            freq[word] = 1\n\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_freq = sorted(freq, key = freq.get, reverse = True)\nsorted_freq.remove('')\n\n#Now we have to tokenize\nvocab2int = {word: ii + 1 for ii, word in enumerate(sorted_freq)}\nint2vocab = {ii + 1: word for ii, word in enumerate(sorted_freq)}\n\nprint(dict(itertools.islice(vocab2int.items(), 100)))","execution_count":13,"outputs":[{"output_type":"stream","text":"{'||Numeric||': 1, 'fire': 2, 'amp': 3, 'im': 4, 'will': 5, 'bomb': 6, 'new': 7, 'via': 8, 'now': 9, 'one': 10, 'peopl': 11, 'go': 12, 'dont': 13, 'news': 14, 'burn': 15, 'kill': 16, 'video': 17, 'flood': 18, 'us': 19, 'emerg': 20, 'crash': 21, 'time': 22, 'disast': 23, 'attack': 24, 'bodi': 25, 'build': 26, 'year': 27, 'look': 28, 'say': 29, 'polic': 30, 'fatal': 31, 'home': 32, 'day': 33, 'love': 34, 'famili': 35, 'evacu': 36, 'train': 37, 'make': 38, 'still': 39, 'come': 40, 'see': 41, 'storm': 42, 'california': 43, 'back': 44, 'know': 45, 'suicid': 46, 'want': 47, 'watch': 48, 'collaps': 49, 'world': 50, 'live': 51, 'bag': 52, 'scream': 53, 'derail': 54, 'got': 55, 'car': 56, 'death': 57, 'man': 58, 'first': 59, 'rt': 60, 'take': 61, 'think': 62, 'caus': 63, 'cant': 64, 'need': 65, 'nuclear': 66, 'work': 67, 'wreck': 68, 'war': 69, 'drown': 70, 'two': 71, 'today': 72, 'youtub': 73, 'destroy': 74, 'accid': 75, 'let': 76, 'deton': 77, 'dead': 78, 'feel': 79, 'plan': 80, 'hijack': 81, 'full': 82, 'fuck': 83, 'obliter': 84, 'good': 85, 'help': 86, 'fear': 87, 'surviv': 88, 'weapon': 89, 'last': 90, 'murder': 91, 'hiroshima': 92, 'may': 93, 'wound': 94, 'life': 95, 'even': 96, 'mani': 97, 'way': 98, 'wildfir': 99, 'get': 100}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_list = change_number(stem_porter(remove_http(remove_stopwords(remove_punct(phrase.lower())))))\nprint(\"Before: \", str(phrase_list))\ndef tokenize_text(text_list):\n    \n    int_text = []\n    for word in text_list:\n        try:\n            int_text.append(vocab2int[word])\n        except:\n            int_text.append(0)\n            \n    return int_text\n\nprint(\"After: \", str(tokenize_text(phrase_list)))\n\nprint(\"Second test: \")\ncommon_list = [\"fire\", \"bomb\", \"somemorerandom,,!!\", \"im\", \"first\", \"jibberish\", \"the\", \"||Numeric||\"]\nprint(tokenize_text(common_list))","execution_count":14,"outputs":[{"output_type":"stream","text":"Before:  ['hello', 'tweet', 'number', '||Numeric||', 'want', 'want', 'tri', 'read', 'tweet', 'write', 'see', '||Numeric||', 'mayb', '||Numeric||', 'time', 'see', 'thisisfun']\nAfter:  [1299, 556, 647, 1, 47, 47, 164, 128, 556, 993, 41, 1, 596, 1, 22, 41, 0]\nSecond test: \n[2, 6, 0, 4, 59, 0, 0, 1]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(lambda x: tokenize_text(x))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"                                                text  target\n0               [3641, 469, 215, 93, 1471, 2883, 19]       1\n1               [141, 2, 186, 504, 5232, 5233, 1026]       1\n2  [1345, 505, 1810, 373, 5234, 220, 36, 1810, 37...       1\n3                  [1, 11, 2401, 99, 36, 331, 43, 0]       1\n4  [55, 1027, 142, 3642, 1620, 197, 99, 2402, 123...       1\n5  [2403, 198, 0, 43, 1346, 1, 332, 782, 486, 867...       1\n6  [18, 23, 708, 175, 63, 709, 18, 438, 5235, 868...       1\n7                        [4, 187, 1098, 41, 2, 1811]       1\n8                [257, 20, 36, 199, 9, 26, 710, 438]       1\n9                            [4, 2064, 334, 40, 241]       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[3641, 469, 215, 93, 1471, 2883, 19]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[141, 2, 186, 504, 5232, 5233, 1026]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1345, 505, 1810, 373, 5234, 220, 36, 1810, 37...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1, 11, 2401, 99, 36, 331, 43, 0]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[55, 1027, 142, 3642, 1620, 197, 99, 2402, 123...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[2403, 198, 0, 43, 1346, 1, 332, 782, 486, 867...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[18, 23, 708, 175, 63, 709, 18, 438, 5235, 868...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[4, 187, 1098, 41, 2, 1811]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[257, 20, 36, 199, 9, 26, 710, 438]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[4, 2064, 334, 40, 241]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 0\nfor tweet in train_df[\"text\"]:\n    if len(tweet) >= max_length:\n        max_length = len(tweet)\n\n        \nprint(\"Max Length: \", str(max_length))    \n    ","execution_count":17,"outputs":[{"output_type":"stream","text":"Max Length:  51\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_text(int_list, sequence_length = 52):\n    \n    padded_list = np.zeros((sequence_length), dtype = int)\n    padded_list[-len(int_list):] = np.array(int_list)[:sequence_length]\n    \n    return padded_list\n\n\nint_list = tokenize_text(phrase_list)\nprint(pad_text(int_list))\n","execution_count":18,"outputs":[{"output_type":"stream","text":"[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0 1299  556  647    1   47   47  164\n  128  556  993   41    1  596    1   22   41    0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have some rows we need to drop or else the pad won't work\nlist_of_empty_rows = []\nfor ii, tweet in enumerate(train_df[\"text\"]):\n    if len(tweet) == 0:\n        list_of_empty_rows.append(ii)\n\nprint(list_of_empty_rows)     \ntrain_df = train_df.drop(list_of_empty_rows)\nprint(\"Rows have been dropped\")","execution_count":19,"outputs":[{"output_type":"stream","text":"[6594, 6597, 6602, 6618, 6620, 6623, 6626]\nRows have been dropped\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(lambda x: pad_text(x))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"First Review\")\nprint(train_df[\"text\"][0][:])\n\nprint(\"Next Review\")\nprint(train_df[\"text\"][1028][:])\n\nprint(\"Next Review\")\nprint(train_df[\"text\"][456][:])\n\nprint(\"Length of review: \", str(len(train_df[\"text\"][906])))","execution_count":21,"outputs":[{"output_type":"stream","text":"First Review\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0 3641  469  215   93 1471 2883   19]\nNext Review\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0   45  513   25\n  896    1   52 3824 3100 1303 1472 3101 6402 3101]\nNext Review\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0 5688\n    0   19 1185   24   16    1  730  532 5689 5690]\nLength of review:  52\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Now I have to create a generator and also split into Valid and Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text(csv_column):\n    listed_data = []\n    for row in csv_column:\n        listed_data.append(row)\n        \n    return listed_data\n\ndef get_target(csv_column):\n    listed_data = []\n    for row in csv_column:\n        listed_row = [row]\n        listed_data.append(listed_row)\n    return listed_data","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = get_text(train_df[\"text\"])\ntrain_y = get_target(train_df[\"target\"])\n\nassert len(train_y) == len(train_x)\nprint(len(train_x))\nprint(len(train_y))","execution_count":23,"outputs":[{"output_type":"stream","text":"7606\n7606\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Can use this method to split data if needed\n\n    def split_data(train_x, train_y):\n    \n    split_index = int(len(train_x) * 0.995)\n    train_x, valid_x = train_x[:split_index], train_x[split_index:]\n    train_y, valid_y = train_y[:split_index], train_y[split_index:]\n    \n    return train_x, train_y, valid_x, valid_y \n\n```  train_x, train_y, valid_x, valid_y = split_data(train_x, train_y) ``` "},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(train_x) == len(train_y)\nprint(len(train_x))\n#Uncomment these lines to create datalaoder for validation\n# assert len(valid_x) == len(valid_y)\n# print(len(valid_x))","execution_count":24,"outputs":[{"output_type":"stream","text":"7606\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataloader(train_x, train_y, batch_size = 30):\n    \n    #Make sure to convert from Numpy to Torch Tensor\n    train_dataset = TensorDataset(torch.LongTensor(train_x), torch.FloatTensor(train_y))\n    \n    train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n    \n    return train_loader\n","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets create a dataloader\ntrain_loader = create_dataloader(train_x[:7595], train_y[:7595])","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at the batches of data\ntrain_loader_iter = iter(train_loader)\nnext(train_loader_iter)[0].shape","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"torch.Size([30, 52])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Now we are going to define the model class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n    \n    def __init__(self,vocab_size, embedding_dim, hidden_size, n_layers):\n        super(LSTM, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm1 = nn.LSTM(embedding_dim, self.hidden_size, num_layers = self.n_layers, dropout = 0.2, batch_first = True)\n        self.fc1 = nn.Linear(self.hidden_size, 1)\n        \n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        \n        batch_size = x.size(0)\n        sequence_len = x.size(1)\n        \n        embeddings = self.embedding(x)\n        lstm_out, hidden = self.lstm1(embeddings)\n        output = lstm_out.contiguous().view(-1, self.hidden_size)\n        \n        output = self.sigmoid(self.fc1(output))\n        output = output.reshape(batch_size, sequence_len, -1)\n        output = output[:, -1]\n        \n        \n        return hidden, output\n    \n    def init_hidden(self, batch_size):\n        \n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda())\n            \n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_size).zero_())\n            \n        return hidden\n        \n        \n        ","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we have to define the train function\n* forward and backprop function\n* The final train function "},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_and_backprop(rnn, optimizer, tweet, target, criterion, hidden):\n    \n    if train_on_gpu:\n        rnn.cuda()\n    \n    hidden = ([each.data for each in hidden])\n    optimizer.zero_grad()\n    \n    hidden, output = rnn(tweet, hidden)\n    loss = criterion(output, target)\n    \n    loss.backward()\n    \n    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n    optimizer.step()\n    \n    loss_for_batch = loss.item()\n    \n    return loss_for_batch, hidden\n    \n    \n    ","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(rnn, epochs, optimizer, criterion, batch_size, train_loader):\n    \n    rnn.train()\n    if train_on_gpu:\n        rnn = rnn.cuda()\n        \n    for epoch in range(1, epochs + 1):\n        \n        hidden = rnn.init_hidden(batch_size)\n        train_loss = 0\n        \n        for batch_i, (tweet, target) in enumerate(train_loader):\n            \n            n_batches = len(train_loader.dataset)//batch_size\n            if(batch_i > n_batches):\n                break\n            \n            if train_on_gpu:\n                tweet = tweet.cuda()\n                target = target.cuda() \n            \n            batch_loss, batch_hidden = forward_and_backprop(rnn, optimizer, tweet, target, criterion, hidden)\n            train_loss += batch_loss\n        \n        \n        print(\"Epoch Number: \", str(epoch)) \n        print(\"Train Loss: \", str(train_loss))\n        \n    \n    \n        \n    \n    ","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now Let us define the hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nbatch_size = 30\nlr = 0.001\n\nvocab_size = len(vocab2int)\nembedding_dim = 230\nhidden_size = 250\nnum_layers = 2\n\nrnn = LSTM(vocab_size, embedding_dim, hidden_size, num_layers) \n\noptimizer = optim.Adam(rnn.parameters(), lr = lr)\ncriterion = nn.BCELoss()","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(rnn, epochs, optimizer, criterion, batch_size, train_loader)","execution_count":32,"outputs":[{"output_type":"stream","text":"Epoch Number:  1\nTrain Loss:  142.83650508522987\nEpoch Number:  2\nTrain Loss:  101.51655912399292\nEpoch Number:  3\nTrain Loss:  68.53155351430178\nEpoch Number:  4\nTrain Loss:  38.91419133543968\nEpoch Number:  5\nTrain Loss:  23.600512051023543\nEpoch Number:  6\nTrain Loss:  19.399995845509693\nEpoch Number:  7\nTrain Loss:  14.735474395100027\nEpoch Number:  8\nTrain Loss:  15.630562658014242\nEpoch Number:  9\nTrain Loss:  11.853240346419625\nEpoch Number:  10\nTrain Loss:  11.097251099796267\nEpoch Number:  11\nTrain Loss:  11.177550852102286\nEpoch Number:  12\nTrain Loss:  10.18252025959373\nEpoch Number:  13\nTrain Loss:  7.811310794848396\nEpoch Number:  14\nTrain Loss:  8.59104484825366\nEpoch Number:  15\nTrain Loss:  7.5842379529094615\nEpoch Number:  16\nTrain Loss:  7.079321550174427\nEpoch Number:  17\nTrain Loss:  8.68538865122082\nEpoch Number:  18\nTrain Loss:  11.517663090286078\nEpoch Number:  19\nTrain Loss:  9.071310374762106\nEpoch Number:  20\nTrain Loss:  8.210935624083504\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Now we are going to decide the predict model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(net, test_review):\n    \n    assert len(test_review) > 0\n    \n    net.eval()\n    preprocessed_data = tokenize_text(preprocess_text(test_review))\n    \n    assert len(test_review) > 0\n    \n    padded_data = pad_text(preprocessed_data)\n    padded_data = padded_data.reshape(1, -1)\n    padded_data = torch.from_numpy(padded_data)\n   \n    \n    batch_size = padded_data.size(0)\n    hidden = net.init_hidden(batch_size)\n    \n    if train_on_gpu:\n        padded_data = padded_data.cuda()\n    \n    hidden, output = net(padded_data, hidden)\n    \n    print(\"Unrounded Answer: \", output)\n    \n    answer = np.round(output.cpu().detach().numpy())\n    if answer == 1:\n        print(\"Call in immediate emergency at location\")\n    else:\n        print(\"General Commentary\")\n    \n    \n\npredict(rnn, \"Breaking News: Flooding on streets\")\npredict(rnn, \"Fire ravaged houses next to me and are approaching me\")\npredict(rnn, \"We the best music. We just chillin #Ballin\")\npredict(rnn, \"Smoke in the air. It smells like smoke #Fire\")\npredict(rnn, \"Smoke in the air. It smells like smoke #SaySikeRightNow\")\npredict(rnn, \"I hear strange noises. The wall is shaking\")\npredict(rnn, \"I am on fire with playing this game #TheGOAT\")\npredict(rnn, \"High winds very high winds the ground is shaking\")\npredict(rnn, \"I dont do domestic violence\")\npredict(rnn, \"The houses next to us have burned to pure ash\")\npredict(rnn, \"Breaking News: High Water levels threatening Silicon Valley\")\npredict(rnn, \"Oh no what is happening. Flooding is affected my House\")\npredict(rnn, \"Oh no what is happening. Flooding is affected my House\")\npredict(rnn, \"there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\")\npredict(rnn, \"This fire is huge. How I am I supposed to put this out myself bruh\")\npredict(rnn, \"I can a broken car set ablaze on the side of street calling 911\")\n\n    ","execution_count":33,"outputs":[{"output_type":"stream","text":"Unrounded Answer:  tensor([[1.0000]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.0006]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[4.2263e-05]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.9999]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.0004]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.0917]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.0009]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[1.0000]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.0006]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[1.0000]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9983]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9965]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9965]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9977]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9997]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9961]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Lets test on some test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(rnn, \"ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF SE VISTA TER GRESHAM OR [Gresham Police #PG15000044357...\")\npredict(rnn, \"@DaveOshry @Soembie So if I say that I met her by accident this week- would you be super jelly Dave?...\")\npredict(rnn, \"We're shaking...It's an earthquake\")\npredict(rnn, \"We are still living in the aftershock of Hiroshima people are still the scars of history.' - Edward...\")\npredict(rnn, \"320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/THyzOMVWU0 | @djicemoon | #Dubstep #TrapMusic #DnB #EDM ...\")\npredict(rnn, \"#UPDATE: Picture from the Penn Twp. airplane accident. http://t.co/6JfgDnZRlC\")\npredict(rnn, \"@thugIauren I had myself on airplane mode by accident ??\")\npredict(rnn, \"Typhoon Soudelor kills 28 in China and Taiwan\")\npredict(rnn, \"No I don't like cold!\")\npredict(rnn, \"Not a diss song. People will take 1 thing and run with it. Smh it's an eye opener though. He is abou...\")\npredict(rnn, \"Just got to love burning your self on a damn curling wand... I swear someone needs to take it away f...\")\npredict(rnn, \"I hate badging shit in accident\")\npredict(rnn, \"Horrible Accident Man Died In Wings of AirplaneåÊ(29-07-2015) http://t.co/5ZRKZdhODe\")\n\n","execution_count":39,"outputs":[{"output_type":"stream","text":"Unrounded Answer:  tensor([[0.3599]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.9996]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9974]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9999]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[7.0223e-05]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[1.0000]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.9396]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[1.0000]], device='cuda:0', grad_fn=<SelectBackward>)\nCall in immediate emergency at location\nUnrounded Answer:  tensor([[0.0007]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.0058]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[5.2274e-05]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.3513]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\nUnrounded Answer:  tensor([[0.0011]], device='cuda:0', grad_fn=<SelectBackward>)\nGeneral Commentary\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}